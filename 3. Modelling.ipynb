{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5b0549",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Capstone: Credit Card Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b9af5",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem Statement\n",
    "You've want to sign up for a credit card that best fits you, but you've realised that you will need to comb through all card descriptions, conditions and reviews in order to decide which card is the best. You find that it is very inconvenient and as a data scientist, you decide to take it upon yourself to build a credit card recommender.\n",
    "\n",
    "Use different techniques to analyse the reviews, credit card conditions to derive a credit card recommender that could help to save a lot of time when choosing a credit card."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b01d0",
   "metadata": {},
   "source": [
    "### Contents:\n",
    "- [Background](#Background)\n",
    "- [Datasets Used](#Datasets-Used)\n",
    "- [Extraction of Data](#Extraction-of-Data)\n",
    "- [Data Import & Cleaning](#Data-Import-and-Cleaning)\n",
    "- [Data Dictionary](#Data-Dictionary)\n",
    "- [Pre-processing text data](#Pre-processing-text-data)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Sentiment Analysis Using BERT](#Sentiment-Analysis-Using-BERT)\n",
    "- [Preparing the dataset for modelling](#Preparing-the-dataset-for-modelling)\n",
    "- [Modelling](#Modelling)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03cce1f",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d50c64",
   "metadata": {},
   "source": [
    "All credit cards in Singapore offer some sort of reward for using them for your purchases, which include incentives like cashback, reward points (which can be traded for air miles, discount vouchers or actual products), or airline miles. However, the most common reward, and perhaps the most enticing, would perhaps be cashback.\n",
    "\n",
    "For the uninitiated, cashback refers to receiving back a percentage of what you spend in the form of money. It is akin to getting a perpetual discount whenever you spend. Sounds too good to be true? It really is not. Credit card companies are constantly competing to provide the most competitive rewards for their customers - some cards offer lucrative sign-up promotions, while others offer higher cashbacks for niche spending categories like travel or sustainability.\n",
    "\n",
    "With so many cards available on the market to choose from, it is no wonder that Singaporeans have a hard time deciding which is the best credit card in Singapore. In particular, it is hard to compare the different cashback rewards across multiple categories for various credit cards. [(source)](https://sg.news.yahoo.com/three-reasons-why-own-credit-104237659.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9440014",
   "metadata": {},
   "source": [
    "## Datasets Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76bff5",
   "metadata": {},
   "source": [
    "Data in this used in the analysis consists of credit card details and reviews scrapped from various websites. Please rerefer to the data dictionary for more information on the columns extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6273032",
   "metadata": {},
   "source": [
    "## Extraction of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5ba53",
   "metadata": {},
   "source": [
    "Please refer to \"**1. Extraction of Data**\" for the steps done for data extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64591a",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06df8f7",
   "metadata": {},
   "source": [
    "Please refer to \"**2. Analysis of Datasets**\" for the steps done for data import and cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd48258",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee89854",
   "metadata": {},
   "source": [
    "<br>**Dataset name: `cashback_expand_df`**\n",
    "<br>This dataset contains all the reviews on cashback credit cards.\n",
    "\n",
    "| Feature | Type | Dataset | Description |\n",
    "|:--|:-:|:-:|:--|\n",
    "|credit_card_name|string|perfumes_df|Name of the credit card.|\n",
    "|card_type|string|perfumes_df|Type of credit card. Cashback of Air Miles.|\n",
    "|reviews|string|perfumes_df|Reviews on the credit cards.|\n",
    "\n",
    "\n",
    "<br>**Dataset name: `miles_expand_df`**\n",
    "<br>This dataset contains all the reviews on air miles credit cards.\n",
    "\n",
    "| Feature | Type | Dataset | Description |\n",
    "|:--|:-:|:-:|:--|\n",
    "|credit_card_name|string|perfumes_df|Name of the credit card.|\n",
    "|card_type|string|perfumes_df|Type of credit card. Cashback of Air Miles.|\n",
    "|reviews|string|perfumes_df|Reviews on the credit cards.|\n",
    "\n",
    "<br>**Dataset name: `combined_expand_df`**\n",
    "<br>This dataset contains all the reviews on all credit cards.\n",
    "\n",
    "| Feature | Type | Dataset | Description |\n",
    "|:--|:-:|:-:|:--|\n",
    "|credit_card_name|string|perfumes_df|Name of the credit card.|\n",
    "|card_type|string|perfumes_df|Type of credit card. Cashback of Air Miles.|\n",
    "|reviews|string|perfumes_df|Reviews on the credit cards.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897c61d",
   "metadata": {},
   "source": [
    "## Pre processing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676d4bd",
   "metadata": {},
   "source": [
    "Please refer to \"**2. Analysis of Datasets**\" for the steps done for pre-processing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ba5cc",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156c413",
   "metadata": {},
   "source": [
    "Please refer to \"**2. Analysis of Datasets**\" for the steps done for exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf5391",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5948c13b",
   "metadata": {},
   "source": [
    "Please refer to \"**2. Analysis of Datasets**\" for the steps done for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da6653",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f61f3b",
   "metadata": {},
   "source": [
    "**1. Importing of libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ff8d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import pickle\n",
    "\n",
    "# Chart plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "\n",
    "# Streamlit\n",
    "import streamlit as st\n",
    "import streamlit.components.v1 as components\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97be30f",
   "metadata": {},
   "source": [
    "**2. Convert features into numerical format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ab09f",
   "metadata": {},
   "source": [
    "In order to perform modelling, we will have to convert our dataset back into multiple-choice format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c94e002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the files you have (adjust the file paths based on your project structure)\n",
    "cashback_questionaire_df = pd.read_csv('dataset/2.cashback_expand_df.csv')\n",
    "miles_questionaire_df = pd.read_csv('dataset/2.miles_expand_df.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "edb03c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (427, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Card</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_cleaned</th>\n",
       "      <th>card_type</th>\n",
       "      <th>Review_tokenized</th>\n",
       "      <th>Review_no_stop</th>\n",
       "      <th>Review_lemmatized</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UOB One Card</td>\n",
       "      <td>I have had my own story of crypto scams due to...</td>\n",
       "      <td>i have had my own story of crypto scams due to...</td>\n",
       "      <td>Cashback</td>\n",
       "      <td>['i', 'have', 'had', 'my', 'own', 'story', 'of...</td>\n",
       "      <td>['story', 'crypto', 'scams', 'due', 'referrals...</td>\n",
       "      <td>['story', 'crypto', 'scam', 'due', 'referral',...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UOB One Card</td>\n",
       "      <td>Good credit card to provide higher interest ra...</td>\n",
       "      <td>good credit card to provide higher interest ra...</td>\n",
       "      <td>Cashback</td>\n",
       "      <td>['good', 'credit', 'card', 'to', 'provide', 'h...</td>\n",
       "      <td>['good', 'credit', 'card', 'provide', 'higher'...</td>\n",
       "      <td>['good', 'credit', 'card', 'provide', 'higher'...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UOB One Card</td>\n",
       "      <td>Applied for this card years back along with on...</td>\n",
       "      <td>applied for this card years back along with on...</td>\n",
       "      <td>Cashback</td>\n",
       "      <td>['applied', 'for', 'this', 'card', 'years', 'b...</td>\n",
       "      <td>['applied', 'card', 'years', 'back', 'along', ...</td>\n",
       "      <td>['applied', 'card', 'year', 'back', 'along', '...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UOB One Card</td>\n",
       "      <td>Pair up this card with the UOB ONE Savings acc...</td>\n",
       "      <td>pair up this card with the uob one savings acc...</td>\n",
       "      <td>Cashback</td>\n",
       "      <td>['pair', 'up', 'this', 'card', 'with', 'the', ...</td>\n",
       "      <td>['pair', 'card', 'uob', 'one', 'savings', 'acc...</td>\n",
       "      <td>['pair', 'card', 'uob', 'one', 'saving', 'acco...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UOB One Card</td>\n",
       "      <td>The longest CC I have. It is a great card to u...</td>\n",
       "      <td>the longest cc i have it is a great card to us...</td>\n",
       "      <td>Cashback</td>\n",
       "      <td>['the', 'longest', 'cc', 'i', 'have', 'it', 'i...</td>\n",
       "      <td>['longest', 'cc', 'great', 'card', 'use', 'use...</td>\n",
       "      <td>['longest', 'cc', 'great', 'card', 'use', 'use...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Card                                             Review  \\\n",
       "0  UOB One Card  I have had my own story of crypto scams due to...   \n",
       "1  UOB One Card  Good credit card to provide higher interest ra...   \n",
       "2  UOB One Card  Applied for this card years back along with on...   \n",
       "3  UOB One Card  Pair up this card with the UOB ONE Savings acc...   \n",
       "4  UOB One Card  The longest CC I have. It is a great card to u...   \n",
       "\n",
       "                                      Review_cleaned card_type  \\\n",
       "0  i have had my own story of crypto scams due to...  Cashback   \n",
       "1  good credit card to provide higher interest ra...  Cashback   \n",
       "2  applied for this card years back along with on...  Cashback   \n",
       "3  pair up this card with the uob one savings acc...  Cashback   \n",
       "4  the longest cc i have it is a great card to us...  Cashback   \n",
       "\n",
       "                                    Review_tokenized  \\\n",
       "0  ['i', 'have', 'had', 'my', 'own', 'story', 'of...   \n",
       "1  ['good', 'credit', 'card', 'to', 'provide', 'h...   \n",
       "2  ['applied', 'for', 'this', 'card', 'years', 'b...   \n",
       "3  ['pair', 'up', 'this', 'card', 'with', 'the', ...   \n",
       "4  ['the', 'longest', 'cc', 'i', 'have', 'it', 'i...   \n",
       "\n",
       "                                      Review_no_stop  \\\n",
       "0  ['story', 'crypto', 'scams', 'due', 'referrals...   \n",
       "1  ['good', 'credit', 'card', 'provide', 'higher'...   \n",
       "2  ['applied', 'card', 'years', 'back', 'along', ...   \n",
       "3  ['pair', 'card', 'uob', 'one', 'savings', 'acc...   \n",
       "4  ['longest', 'cc', 'great', 'card', 'use', 'use...   \n",
       "\n",
       "                                   Review_lemmatized  sentiment  \n",
       "0  ['story', 'crypto', 'scam', 'due', 'referral',...          1  \n",
       "1  ['good', 'credit', 'card', 'provide', 'higher'...          4  \n",
       "2  ['applied', 'card', 'year', 'back', 'along', '...          5  \n",
       "3  ['pair', 'card', 'uob', 'one', 'saving', 'acco...          4  \n",
       "4  ['longest', 'cc', 'great', 'card', 'use', 'use...          5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine the two datasets into one combined dataset\n",
    "combined_questionaire_df = pd.concat([cashback_questionaire_df, miles_questionaire_df], ignore_index=True)\n",
    "\n",
    "# Check the total number of rows to ensure that the dataset has been merged correctly\n",
    "print(f\"Combined dataset shape: {combined_questionaire_df.shape}\")\n",
    "display(combined_questionaire_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c627ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Dataset with One-Hot Encoding:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Card</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_cleaned</th>\n",
       "      <th>Review_tokenized</th>\n",
       "      <th>Review_no_stop</th>\n",
       "      <th>Review_lemmatized</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>card_type_Air Miles</th>\n",
       "      <th>card_type_Cashback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UOB One Card</td>\n",
       "      <td>I have had my own story of crypto scams due to...</td>\n",
       "      <td>i have had my own story of crypto scams due to...</td>\n",
       "      <td>['i', 'have', 'had', 'my', 'own', 'story', 'of...</td>\n",
       "      <td>['story', 'crypto', 'scams', 'due', 'referrals...</td>\n",
       "      <td>['story', 'crypto', 'scam', 'due', 'referral',...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UOB One Card</td>\n",
       "      <td>Good credit card to provide higher interest ra...</td>\n",
       "      <td>good credit card to provide higher interest ra...</td>\n",
       "      <td>['good', 'credit', 'card', 'to', 'provide', 'h...</td>\n",
       "      <td>['good', 'credit', 'card', 'provide', 'higher'...</td>\n",
       "      <td>['good', 'credit', 'card', 'provide', 'higher'...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UOB One Card</td>\n",
       "      <td>Applied for this card years back along with on...</td>\n",
       "      <td>applied for this card years back along with on...</td>\n",
       "      <td>['applied', 'for', 'this', 'card', 'years', 'b...</td>\n",
       "      <td>['applied', 'card', 'years', 'back', 'along', ...</td>\n",
       "      <td>['applied', 'card', 'year', 'back', 'along', '...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UOB One Card</td>\n",
       "      <td>Pair up this card with the UOB ONE Savings acc...</td>\n",
       "      <td>pair up this card with the uob one savings acc...</td>\n",
       "      <td>['pair', 'up', 'this', 'card', 'with', 'the', ...</td>\n",
       "      <td>['pair', 'card', 'uob', 'one', 'savings', 'acc...</td>\n",
       "      <td>['pair', 'card', 'uob', 'one', 'saving', 'acco...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UOB One Card</td>\n",
       "      <td>The longest CC I have. It is a great card to u...</td>\n",
       "      <td>the longest cc i have it is a great card to us...</td>\n",
       "      <td>['the', 'longest', 'cc', 'i', 'have', 'it', 'i...</td>\n",
       "      <td>['longest', 'cc', 'great', 'card', 'use', 'use...</td>\n",
       "      <td>['longest', 'cc', 'great', 'card', 'use', 'use...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Card                                             Review  \\\n",
       "0  UOB One Card  I have had my own story of crypto scams due to...   \n",
       "1  UOB One Card  Good credit card to provide higher interest ra...   \n",
       "2  UOB One Card  Applied for this card years back along with on...   \n",
       "3  UOB One Card  Pair up this card with the UOB ONE Savings acc...   \n",
       "4  UOB One Card  The longest CC I have. It is a great card to u...   \n",
       "\n",
       "                                      Review_cleaned  \\\n",
       "0  i have had my own story of crypto scams due to...   \n",
       "1  good credit card to provide higher interest ra...   \n",
       "2  applied for this card years back along with on...   \n",
       "3  pair up this card with the uob one savings acc...   \n",
       "4  the longest cc i have it is a great card to us...   \n",
       "\n",
       "                                    Review_tokenized  \\\n",
       "0  ['i', 'have', 'had', 'my', 'own', 'story', 'of...   \n",
       "1  ['good', 'credit', 'card', 'to', 'provide', 'h...   \n",
       "2  ['applied', 'for', 'this', 'card', 'years', 'b...   \n",
       "3  ['pair', 'up', 'this', 'card', 'with', 'the', ...   \n",
       "4  ['the', 'longest', 'cc', 'i', 'have', 'it', 'i...   \n",
       "\n",
       "                                      Review_no_stop  \\\n",
       "0  ['story', 'crypto', 'scams', 'due', 'referrals...   \n",
       "1  ['good', 'credit', 'card', 'provide', 'higher'...   \n",
       "2  ['applied', 'card', 'years', 'back', 'along', ...   \n",
       "3  ['pair', 'card', 'uob', 'one', 'savings', 'acc...   \n",
       "4  ['longest', 'cc', 'great', 'card', 'use', 'use...   \n",
       "\n",
       "                                   Review_lemmatized  sentiment  \\\n",
       "0  ['story', 'crypto', 'scam', 'due', 'referral',...          1   \n",
       "1  ['good', 'credit', 'card', 'provide', 'higher'...          4   \n",
       "2  ['applied', 'card', 'year', 'back', 'along', '...          5   \n",
       "3  ['pair', 'card', 'uob', 'one', 'saving', 'acco...          4   \n",
       "4  ['longest', 'cc', 'great', 'card', 'use', 'use...          5   \n",
       "\n",
       "   card_type_Air Miles  card_type_Cashback  \n",
       "0                    0                   1  \n",
       "1                    0                   1  \n",
       "2                    0                   1  \n",
       "3                    0                   1  \n",
       "4                    0                   1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert 'card_type' to one-hot encoded format\n",
    "card_type_onehot = pd.get_dummies(combined_questionaire_df['card_type'], prefix='card_type')\n",
    "\n",
    "# Concatenate the one-hot encoded columns back into the original dataframe\n",
    "combined_questionaire_df = pd.concat([combined_questionaire_df, card_type_onehot], axis=1)\n",
    "\n",
    "# Drop the original 'card_type' column since it's now encoded\n",
    "combined_questionaire_df.drop('card_type', axis=1, inplace=True)\n",
    "\n",
    "# Display the updated dataset\n",
    "print(\"Updated Dataset with One-Hot Encoding:\")\n",
    "display(combined_questionaire_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f1cd9",
   "metadata": {},
   "source": [
    "**3. Convert target column into numerical format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc4bfe",
   "metadata": {},
   "source": [
    "Since the model isn't able to read text, we will also need to convert our target column into numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e346111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing labels after mapping: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Map the card names to numeric labels\n",
    "combined_questionaire_df['label'] = combined_questionaire_df['Card'].map({\n",
    "    'UOB One Card': 1, \n",
    "    'UOB Ladys Card': 2, \n",
    "    'HSBC Revolution Credit Card': 3, \n",
    "    'Citi PremierMiles Visa Card': 4,\n",
    "    'DBS Altitude Visa Signature Card': 5\n",
    "})\n",
    "\n",
    "# Step 2: Check if there are any missing labels after mapping\n",
    "missing_labels = combined_questionaire_df['label'].isna().sum()\n",
    "print(f\"Missing labels after mapping: {missing_labels}\")\n",
    "\n",
    "# Step 3: Handle missing values (if any)\n",
    "combined_questionaire_df['label'].fillna(0, inplace=True)  # Use 0 for unknown cards\n",
    "\n",
    "# Step 4: Plot the count of each label\n",
    "plt.figure(figsize=(10, 6))  # Set the size of the plot\n",
    "label_cat = sns.countplot(x='label', data=combined_questionaire_df, palette='husl')\n",
    "plt.title('Count of Labels')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e68cdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Encode the card names\n",
    "encoder = LabelEncoder()\n",
    "combined_questionaire_df['Card_encoded'] = encoder.fit_transform(combined_questionaire_df['Card'])\n",
    "\n",
    "# Use TfidfVectorizer to transform text data into numerical format\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Limiting features for simplicity\n",
    "X_text = vectorizer.fit_transform(combined_questionaire_df['Review_no_stop'].astype(str))\n",
    "\n",
    "# Combine the numerical features with the encoded card column\n",
    "X = X_text.toarray()  # Convert sparse matrix to dense\n",
    "y = combined_questionaire_df['Card_encoded']  # Target column\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cced9ce0",
   "metadata": {},
   "source": [
    "Now that we have prepared the dataset, let'start with modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc1745e",
   "metadata": {},
   "source": [
    "**4. Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e4dbd",
   "metadata": {},
   "source": [
    "We will use the below models to make the prediction:\n",
    "- K Nearest Neighbour\n",
    "- Random Forest Classifier\n",
    "- Ada Boost Classifier\n",
    "- Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa50842",
   "metadata": {},
   "source": [
    "**K Nearest Neighbour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2886c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Cross Validation Score: 0.666\n",
      "KNN Test Score: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Instantiate and Fit the KNN Model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model using Cross-Validation\n",
    "print(f'KNN Cross Validation Score: {round(cv_score, 3)}')\n",
    "\n",
    "# Step 7: Predict on the test set and evaluate the accuracy\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "print(f'KNN Test Score: {round(accuracy, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269466b",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e0e71c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Validation Score: 0.741\n",
      "Random Forest Test Score: 0.766\n"
     ]
    }
   ],
   "source": [
    "# Train the Random Forest Model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features=\"sqrt\", random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "print(f'Random Forest Cross Validation Score: {round(cv_score_rf, 3)}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Check the test score\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Test Score: {round(test_accuracy_rf, 3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b465b9a",
   "metadata": {},
   "source": [
    "**Ada Boost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bbe887b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boost Cross Validation Score: 0.616\n",
      "Ada Boost Test Score: 0.654\n"
     ]
    }
   ],
   "source": [
    "# Train the Ada Boost Model\n",
    "ab = AdaBoostClassifier(n_estimators=100)\n",
    "ab.fit(X_train, y_train)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "print(f'Ada Boost Cross Validation Score: {round(cross_val_score(ab, X_train, y_train, cv=5).mean(),3)}')\n",
    "\n",
    "# Check the test score\n",
    "y_pred_ab = ab.predict(X_test)\n",
    "print(f'Ada Boost Test Score: {round(accuracy_score(y_test, y_pred_ab),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825b22e",
   "metadata": {},
   "source": [
    "**Gradient Boost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7f8123da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Cross Validation Score: 0.762\n",
      "Gradient Boost Test Score: 0.738\n"
     ]
    }
   ],
   "source": [
    "# Train the Gradient Boost Model\n",
    "gb = GradientBoostingClassifier(n_estimators=100)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "print(f'Gradient Boost Cross Validation Score: {round(cross_val_score(gb, X_train, y_train, cv=5).mean(),3)}')\n",
    "\n",
    "# Check the test score\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(f'Gradient Boost Test Score: {round(accuracy_score(y_test, y_pred_gb),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cda867",
   "metadata": {},
   "source": [
    "**Train-Test Summary Score:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af9258",
   "metadata": {},
   "source": [
    "The scores are as per below:\n",
    "\n",
    "| Model | Train Results | Test Results |\n",
    "|:--|:-:|:-:|\n",
    "|K Nearest Neighbour|0.666|0.71|\n",
    "|**<font color = green>Random Forest</font>**|**<font color = green>0.741</font>**|**<font color = green>0.766</font>**|\n",
    "|Ada Boost|0.616|0.654|\n",
    "|Gradient Boost|0.762|0.738|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bba33b",
   "metadata": {},
   "source": [
    "The first round of train and test results looks rather promosing. The best performing model is the Random Forest Classifier with a train result of 0.943 and a test result of 0.945. Gradient Boost Classifier had a higher train result of 0..953 but there seems to be overfitting as the difference between the train result and test result is more than 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280cccd",
   "metadata": {},
   "source": [
    "**5. Addressing Class Imbalance using Smote**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5fb1b3",
   "metadata": {},
   "source": [
    "SMOTE stands for Synthetic Minority Oversampling Technique. SMOTE is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together. [(source)](https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/)\n",
    "\n",
    "Due to imbalanced data, it is possible to make a model that appears very accurate, while it actually is useless. This is because traditional machine learning models and evaluation metrics that assume a balanced class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a20869c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution before applying SMOTE:\n",
      "4    128\n",
      "1     80\n",
      "0     57\n",
      "2     41\n",
      "3     14\n",
      "\n",
      "\n",
      "Distribution after applying SMOTE:\n",
      "4    128\n",
      "1    128\n",
      "0    128\n",
      "2    128\n",
      "3    128\n"
     ]
    }
   ],
   "source": [
    "# Value counts before applying smote\n",
    "print('Distribution before applying SMOTE:')\n",
    "print(y_train.value_counts().to_string())\n",
    "\n",
    "#Oversampling the data\n",
    "smote = SMOTE(random_state = 42, k_neighbors = 3)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check that smote has been applied to X_train and y_train\n",
    "print('Distribution after applying SMOTE:')\n",
    "print(y_train_smote.value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93005cc8",
   "metadata": {},
   "source": [
    "**K Nearest Neighbour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b82e7da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Cross Validation Score: 0.666\n",
      "KNN Test Score: 0.355\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a class instance of KNN class with an initial parameter value\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "print(f'KNN Cross Validation Score: {round(cross_val_score(knn, X_train, y_train, cv=5).mean(),3)}')\n",
    "\n",
    "# Check the test score\n",
    "print(f'KNN Test Score: {round(knn.score(X_test, y_test),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abe5cd1",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8ae0b37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Validation Score: 0.741\n",
      "Random Forest Test Score: 0.766\n"
     ]
    }
   ],
   "source": [
    "# Train the Random Forest Model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features=\"sqrt\", random_state=42)\n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "cv_score_rf = cross_val_score(rf, X_train, y_train, cv=5).mean()\n",
    "print(f'Random Forest Cross Validation Score: {round(cv_score_rf, 3)}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Check the test score\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f'Random Forest Test Score: {round(test_accuracy_rf, 3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442803c7",
   "metadata": {},
   "source": [
    "**Ada Boost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3b74c5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boost Cross Validation Score: 0.616\n",
      "Ada Boost Test Score: 0.626\n"
     ]
    }
   ],
   "source": [
    "# Train the Ada Boost Model\n",
    "ab = AdaBoostClassifier(n_estimators=100)\n",
    "ab.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "print(f'Ada Boost Cross Validation Score: {round(cross_val_score(ab, X_train, y_train, cv=5).mean(),3)}')\n",
    "\n",
    "# Check the test score\n",
    "y_pred_ab = ab.predict(X_test)\n",
    "print(f'Ada Boost Test Score: {round(accuracy_score(y_test, y_pred_ab),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced96c5",
   "metadata": {},
   "source": [
    "**Gradient Boost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9721c192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Cross Validation Score: 0.775\n",
      "Gradient Boost Test Score: 0.757\n"
     ]
    }
   ],
   "source": [
    "# Train the Gradient Boost Model\n",
    "gb = GradientBoostingClassifier(n_estimators=100)\n",
    "gb.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "print(f'Gradient Boost Cross Validation Score: {round(cross_val_score(gb, X_train, y_train, cv=5).mean(),3)}')\n",
    "\n",
    "# Check the test score\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(f'Gradient Boost Test Score: {round(accuracy_score(y_test, y_pred_gb),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe6f98",
   "metadata": {},
   "source": [
    "**Train-Test Summary Score (After Applying Smote):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b4a3c0",
   "metadata": {},
   "source": [
    "| Model | Train Results | Test Results |\n",
    "|:--|:-:|:-:|\n",
    "| K Nearest Neighbour Classifier | 0.666 | 0.355 |\n",
    "| **<font color = green>Random Forest Classifier</font>** | **<font color = green>0.741</font>** | **<font color = green>0.766</font>** |\n",
    "| Ada Boost Classifier | 0.616 | 0.626 |\n",
    "| Gradient Boost Classifier | 0.775 | 0.757 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb04a8",
   "metadata": {},
   "source": [
    "After applying SMOTE, the all the test scores have decreased. This could be due to the class imbalance which have been addressed by applying SMOTE. Gradient Boost Classifier seems to be the best performing model now with a train result of 0.953 and a test result of 0.938. Ada Boost Classifier seems to be severly overfitted as the difference between the train result and test result is almost 0.3.\n",
    "\n",
    "Now, let's apply GridSearchCV to further optimise the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842cefb",
   "metadata": {},
   "source": [
    "**6. Optimising the models using GridSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7af99",
   "metadata": {},
   "source": [
    "**K Nearest Neighbour**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "609d5eaa",
   "metadata": {},
   "source": [
    "knn_params = {\n",
    "    'n_neighbors': [5, 10, 30, 50, 100, 200, 300],                      \n",
    "    'weights': ['distance', 'uniform'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],                   \n",
    "    'leaf_size': [10, 20, 30, 50, 100],\n",
    "    'p': [1, 2]\n",
    "    }\n",
    "    \n",
    "model = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)\n",
    "\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "print(\"Best param value:\")\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8e9b7",
   "metadata": {},
   "source": [
    "We have ran the above code and have obtained the these as the best parameters for K Nearest Neighbour:\n",
    "\n",
    "`{'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 5, 'p': 1, 'weights': 'distance'}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa3454",
   "metadata": {},
   "source": [
    "We will load the above parameters into K Nearest Neighbour to generate the optimised model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e9f00016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Cross Validation Score: 0.684\n",
      "KNN Test Score: 0.607\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a class instance of KNN class with an initial parameter value\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights = 'distance', algorithm = 'auto', leaf_size = 10, p = 1 )\n",
    "knn.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "print(f'KNN Cross Validation Score: {round(cross_val_score(knn, X_train, y_train, cv=5).mean(),3)}')\n",
    "\n",
    "# Check the test score\n",
    "print(f'KNN Test Score: {round(knn.score(X_test, y_test),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a975ae9e",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cd5b4b0",
   "metadata": {},
   "source": [
    "rf_params = {\n",
    "    'n_estimators':[10, 20, 50, 100, 200, 300],                     \n",
    "    'max_depth':[10, 20, 30, 40, 50, 100, 150],                         \n",
    "    'min_samples_leaf': [1, 5, 10, 20]\n",
    "    }\n",
    "\n",
    "model = GridSearchCV(RandomForestClassifier(), rf_params, cv=5)\n",
    "\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "print(\"Best param value:\")\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d774e",
   "metadata": {},
   "source": [
    "We have ran the above code and have obtained the these as the best parameters for Random Forest Classifier:\n",
    "\n",
    "`{'max_depth': 50, 'min_samples_leaf': 1, 'n_estimators': 300}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aacb2dd",
   "metadata": {},
   "source": [
    "We will load the above parameters into Random Forest Classifier to generate the optimised model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "65ce0cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross Validation Score: 0.741\n",
      "Random Forest Test Score: 0.766\n"
     ]
    }
   ],
   "source": [
    "# Train the Random Forest Model\n",
    "rfr = RandomForestClassifier(n_estimators=300, max_features=\"log2\", max_depth = 50, \n",
    "                            min_samples_leaf = 1, random_state=42)\n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "print(f'Random Forest Cross Validation Score: {round(cross_val_score(rf, X_train, y_train, cv=5).mean(),3)}')\n",
    "\n",
    "# Check the test score\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(f'Random Forest Test Score: {round(accuracy_score(y_test, y_pred_rf),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0477f",
   "metadata": {},
   "source": [
    "**Ada Boost Classifier**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95b226af",
   "metadata": {},
   "source": [
    "ada_params = {\n",
    "    'n_estimators':[10, 50, 100, 200, 300],                     \n",
    "    'learning_rate':[1, 2, 3, 5, 10, 20, 30]\n",
    "    }\n",
    "\n",
    "model = GridSearchCV(AdaBoostClassifier(), ada_params, cv=5)\n",
    "\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "print(\"Best param value:\")\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee6531",
   "metadata": {},
   "source": [
    "We have ran the above code and have obtained the these as the best parameters for Ada Boost Classifier:\n",
    "\n",
    "`{'learning_rate': 1, 'n_estimators': 300}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e89c1",
   "metadata": {},
   "source": [
    "We will load the above parameters into Random Forest Classifier to generate the optimised model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c26be2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boost Cross Validation Score: 0.625\n",
      "Ada Boost Test Score: 0.607\n"
     ]
    }
   ],
   "source": [
    "# Train the Ada Boost Model\n",
    "ab = AdaBoostClassifier(n_estimators=300, learning_rate = 1)\n",
    "ab.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "print(f'Ada Boost Cross Validation Score: {round(cross_val_score(ab, X_train, y_train, cv=5).mean(),3)}')\n",
    "\n",
    "# Check the test score\n",
    "y_pred_ab = ab.predict(X_test)\n",
    "print(f'Ada Boost Test Score: {round(accuracy_score(y_test, y_pred_ab),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded0db0",
   "metadata": {},
   "source": [
    "**Gradient Boost Classifier**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29030612",
   "metadata": {},
   "source": [
    "gb_params = {\n",
    "    'loss':['log_loss', 'exponential'],                     \n",
    "    'learning_rate': [0.1, 0.2, 0.3, 0.4],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.0, 0.5, 1]\n",
    "    }\n",
    "\n",
    "model = GridSearchCV(GradientBoostingClassifier(), gb_params, cv=5)\n",
    "\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "print(\"Best param value:\")\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9913ae0",
   "metadata": {},
   "source": [
    "We have ran the above code and have obtained the these as the best parameters for Gradient Boost Classifier:\n",
    "\n",
    "`{'learning_rate': 0.4, 'loss': 'log_loss', 'n_estimators': 200, 'subsample': 0.5}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904b4db",
   "metadata": {},
   "source": [
    "We will load the above parameters into Gradient Boost Classifier to generate the optimised model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "63b5298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost Cross Validation Score: 0.778\n",
      "Gradient Boost Test Score: 0.738\n"
     ]
    }
   ],
   "source": [
    "# Train the Gradient Boost Model\n",
    "gb = GradientBoostingClassifier(n_estimators = 200, loss = 'log_loss', subsample = 0.5, learning_rate = 0.4)\n",
    "gb.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Derive the cross-val score for training set\n",
    "print(f'Gradient Boost Cross Validation Score: {round(cross_val_score(gb, X_train, y_train, cv=5).mean(),3)}')\n",
    "\n",
    "# Check the test score\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(f'Gradient Boost Test Score: {round(accuracy_score(y_test, y_pred_gb),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf00827",
   "metadata": {},
   "source": [
    "**Train-Test Summary Score (After Performing GridSearchCV):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30514841",
   "metadata": {},
   "source": [
    "| Model | Train Results | Test Results |\n",
    "|:--|:-:|:-:|\n",
    "|K Nearest Neighbour|0.684|0.607|\n",
    "|**<font color = green>Random Forest Classifier</font>**|**<font color = green>0.741</font>**|**<font color = green>0.766</font>**|\n",
    "|Ada Boost Classifier|0.625|0.607|\n",
    "|Gradient Boost Classifier|0.778|0.738|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5e164",
   "metadata": {},
   "source": [
    "After performing GridSearchCV, Gradient Boost Classifier has the best performing train and test with a result of 0.778 and 0.738 respectively. However, we would choose Random Forest Classifier as our preferred choice of model. This is because the restuls for the Random Forest Classifier is also relatively good with a train and test result of 0.741 and 0.766 respectively. However, the model takes considerably lesser time to run. Considering that we will be deploying this on a live website, a faster model will be preferred as user experience will definitely be better. Since the results are not significantly different, we would choose the slightly less accurate but faster model as our preferred choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846159a",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f71aa9b",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "- Recommender deployed successfully on https://keycards.herokuapp.com/\n",
    "- The recommender is fuss free way to select the best suited credit cards. The recommender uses a Random Forest Model with a model accuracy of 77%.\n",
    "- Besides card return, customers service is also quite an important factor when determining a good credit card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce4c64",
   "metadata": {},
   "source": [
    "### Limitations and Future Enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709f7fe",
   "metadata": {},
   "source": [
    "- Analysis is only limited to all the reviews that have been successfully scrapped. Suggest extracting reviews from other sites as well.\n",
    "- Recommender only has 8 cards at the moment. To include more credit cards in the future.\n",
    "- Recommender could be scaled to include credit cards for other countries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
